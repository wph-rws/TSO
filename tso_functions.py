import glob
import os
import re
import numpy as np
import pandas as pd
import pathlib
import yaml
import pyproj

from read_individual_files import read_individual_files

#%% Load all files

def generate_filelist(location, measurement_date='latest'):
    
    _, _, location_data = get_location_info(location)

    workdir = pathlib.Path.cwd() / f'{location_data}'
    all_dirs = sorted(glob.glob(str(workdir)+'/*/', recursive = True))
    
    # Filter all map names where the last part has 8 digits, e.g. 20240401
    filtered_dirs = [d for d in all_dirs if re.match(r'^.*[\\/]\d{8}', d)]
    filtered_dirs = pd.DataFrame(filtered_dirs, columns=['Meting_id'])
    
    # Find measurement date in list of directories, e.g. '20240601'
    dirname = find_dirname_measurement(location, measurement_date)
        
    # List of all files from most recent directory
    filelist = sorted(glob.glob(os.path.join(dirname, 'F*.dat')))
    filelist = pd.DataFrame(filelist, columns=['filename'])
    filelist['fn_short'] = filelist['filename'].apply(lambda x: pathlib.Path(x).stem)
    
    # Assumption is that filenames F**** have been generated by rename_raw_tso_files
    # First convert number to integer without leading zeros, then to string
    filelist['loc_num'] = filelist['fn_short'].str.strip('F').values.astype(int).astype(str)
    
    return filelist

#%%

# Function to get all directories with indiviudal measurement files
def get_all_datafiles(location, plot_mode='single'):
    # Get location code
    location_code, _ , location_data = get_location_info(location)    
   
    project_dirs = load_project_dirs()
    data_dir = pathlib.Path(project_dirs.get('data_dir'))
    
    # Grevelingen Noord is in same directory as Grevelingen
    if location == 'grno':
        workdir_vert = data_dir / f'{location_data}/'
    else:
        workdir_vert = data_dir / f'{location}/'
    
    # Find all datafiles; for plot_mode 'single', include files that might have extra characters after the date, 
    # like '_03' when only three measurements are available
    if plot_mode == 'single':
        all_datafiles = glob.glob(str(workdir_vert) + f'/{location_code}????????*')
    else:
        all_datafiles = glob.glob(str(workdir_vert) + f'/{location_code}????????')
    
    filelist_datafiles = pd.DataFrame(data=all_datafiles, columns=['Meting']).sort_values(by='Meting').reset_index(drop=True)
    filelist_datafiles['Meting_id'] = filelist_datafiles['Meting'].apply(lambda x: pathlib.Path(x).stem)
    
    # Define the valid pattern: location_code followed by exactly 8 digits, optionally followed by additional characters.
    valid_pattern = re.compile(rf"^{location_code}\d{{8}}.*$")
    
    # Filter the dataframe to keep only files that match the expected pattern.
    filelist_datafiles = filelist_datafiles[filelist_datafiles['Meting_id'].str.match(valid_pattern)]
    
    if filelist_datafiles.empty:
        raise ValueError(f'Geen datafiles gevonden voor locatie "{location_code}" in map "{workdir_vert}"')
    
    return filelist_datafiles

#%%

# Function to get all directories with indiviudal measurement files
def get_measurement_dirs(location):
        
    # Get location code
    _, _, location_data = get_location_info(location)
    
    # Load location of individual files
    project_dirs = load_project_dirs()
    individual_files_dir = pathlib.Path(project_dirs.get('individual_files_dir'))

    workdir_individual_files = individual_files_dir / f'{location_data}/'
    
    # Find all datafiles
    all_dirs = glob.glob(str(workdir_individual_files) + '/????????*/')
    list_measurement_dirs = pd.DataFrame(data=all_dirs, columns=['Meting']).sort_values(by='Meting').reset_index(drop=True)
    list_measurement_dirs['Meting_id'] = list_measurement_dirs['Meting'].apply(lambda x: pathlib.Path(x).stem)
    
    return list_measurement_dirs

#%%

def find_dirname_measurement(location, measurement_date='latest'):
    list_measurement_dirs = get_measurement_dirs(location)
    
    if measurement_date == 'latest':
        
        # Use the last entry
        dirname = list_measurement_dirs['Meting'].iloc[-1]
    
    elif isinstance(measurement_date, int):
        
        # Use the integer as an index (this supports negative indices as well)
        try:
            dirname = list_measurement_dirs['Meting'].iloc[measurement_date]
        except IndexError as e:
            raise ValueError(f"Index {measurement_date} is out of range for the measurement directories.") from e
    
    else:
        try:
            # Assume measurement_date is a string to be matched in the 'Meting_id' column
            matched_dirs = list_measurement_dirs['Meting'][list_measurement_dirs['Meting_id'] == f'{measurement_date}']
            num_matches = len(matched_dirs)
            
            if num_matches == 0:
                raise ValueError(f'No matches found for "{measurement_date}"')
            elif num_matches > 1:
                raise ValueError(f'Multiple matches found for "{measurement_date}"')
        except ValueError as e:
            print(e)            
            raise
        
        # If a match was found, extract the directory name
        dirname = matched_dirs.iloc[0]
    
    return dirname



#%%

def find_filename_datafile(location, measurement_date='latest', plot_mode='single'):
    
    filelist_datafiles = get_all_datafiles(location, plot_mode)
    
    if measurement_date == 'latest':
        filename_datafile = filelist_datafiles['Meting'].iloc[-1]
    
    # Check if the measurement_date is an integer (e.g. -2 or -3)
    elif isinstance(measurement_date, (int, np.integer)):
        try:
            # Handle negative or positive indices
            if len(filelist_datafiles) >= np.abs(measurement_date):
                filename_datafile = filelist_datafiles['Meting'].iloc[measurement_date]
            else:
                raise IndexError(f'Index {measurement_date} is out of bounds for available files.')
        except IndexError as e:
            print(e)
            return None   
    
    # Match the string against the meting_id column 
    else:      
        try:                       
            location_code, _ , _ = get_location_info(location)
            filename_datafile = filelist_datafiles['Meting'][filelist_datafiles['Meting_id'] == f'{location_code}{measurement_date}']
            
            # Check the number of matches
            num_matches = len(filename_datafile)
            
            if num_matches == 0:
                raise ValueError(f'No matches found for "{measurement_date}"')
            elif num_matches > 1:
                raise ValueError(f'Multiple matches found for "{measurement_date}"')
    
        except ValueError as e:
            print(e)
            return None
        
        # Get string from series
        filename_datafile = filename_datafile.iloc[0]
            
    return filename_datafile

#%% 

# Function to get location code or location name from location
def get_location_info(location):
    
    # Load data from YAML file
    with open('locaties.yaml', 'r') as file:
        df = pd.DataFrame(yaml.safe_load(file)['locations'])
    
    # Filter the DataFrame for the given location name
    result = df[df['location'] == location]
    location_code = result['location_code'].values[0]
    location_name = result['location_name'].values[0]
    location_data = result['location_data'].values[0]
    
    if not result.empty:
        return location_code, location_name, location_data
    else:
        raise ValueError(f'File mpnaam of {location} not found')
        
#%%

class MPNaam:
    def __init__(self, location):
        self.location = location
        self.df = self.read_mpnaam()

    def read_mpnaam(self):
        
        # Retrieve location data
        _, _, location_data = get_location_info(self.location)
        
        # Load directory for individual files
        project_dirs = load_project_dirs()
        indiv_files_dir = pathlib.Path(project_dirs.get('individual_files_dir'))
        
        # Load measurement points from CSV file
        df = pd.read_csv(indiv_files_dir / f'{location_data}/mpnaam', header=None, sep='\\s+', names=['x', 'y', 'max_depth', 'loc_name'])
        
        # Convert values to string (if not already string from reading csv)
        df['loc_name'] = df['loc_name'].astype(str)       
              
        if self.location in ('grev', 'grno'):
            
            # Drop 'DREI', will be corrected later in read_datafile
            # if 'DREI' is found instead of 'DREIS'            
            df = df[df['loc_name'] != 'DREI'].reset_index(drop=True)
            
            if self.location == 'grev':
                df = df.iloc[:20]
            else:
                df = df.iloc[20:]                
            
        # Set distances for VZM locations
        if self.location in ('anka', 'volk', 'zoom'):
            
            # Create dictionary with location key and location order
            location_order = {
                'anka': np.array([37, 39, 40, 41, 42, 43, 44, 45, 46]).astype(str),
                'volk': np.array([15, 16, 13, 12, 11, 10, 9, 7, 4, 1]).astype(str),
                'zoom': np.array([27, 29, 30, 34, 33, 32, 31, 36, 37, 38]).astype(str)
            }            
            
            # Filter for locations that are in use
            df = df[df['loc_name'].isin(location_order[self.location])]
            
            # Create a sorting DataFrame
            sort_df = pd.DataFrame({'loc_name': location_order[self.location], 'order': range(len(location_order[self.location]))})
            sort_df = sort_df.set_index('loc_name')
            
            # Merge with the sorting DataFrame
            df = df.merge(sort_df, on='loc_name', how='left')
            
            # Sort based on the 'order' column
            df = df.sort_values('order')  
            
            # Insert location numbers
            df.insert(3, 'loc_num', np.array(location_order[self.location]).astype(int))
            
            self.location_order = location_order
            
        # Insert location numbers for all other locations    
        if not self.location in ('anka', 'volk', 'zoom'):
        
            start_mp_num = int(df['loc_name'].iloc[0])
            end_mp_num = start_mp_num + df.shape[0]
            
            self.location_order = {self.location: np.arange(start_mp_num, end_mp_num)}
            df.insert(3, 'loc_num', np.arange(start_mp_num, end_mp_num))
                    
        # Calculate distances between measurement points
        df['delta'] = np.sqrt(df['x'].diff().abs()**2 + df['y'].diff().abs()**2)
        df['dist'] = df['delta'].cumsum().round()
        
        # Replace NaNs by zeros for interpolation to work
        df[['delta', 'dist']] = df[['delta', 'dist']].fillna(0)
        df.index = df['loc_num']

        # Convert RD New coordinates to ETRS89 / UTM zone 31N
        transformer = pyproj.Transformer.from_crs('EPSG:28992', 'EPSG:25831', always_xy=True)
        df[['x_etrs', 'y_etrs']] = df.apply(
            lambda row: pd.Series(transformer.transform(row['x'], row['y'])), axis=1)

        return df

# Example usage:
# mpnaam = MPNaam('grev')
# print(mpnaam.df)

#%%

# Function that loads all predefined parameters
def load_tso_parameters():

    fn_tso_parameters = 'tso_parameters.yaml' 

    # Load the TSO parameters from YAML file
    with open(fn_tso_parameters, 'r') as file:
        parameters = yaml.safe_load(file)
        
    return parameters

# Function that loads the project directories
def load_project_dirs():
    
    fn_project_dirs = 'project_dirs.yaml'
    
    # Load directory paths from YAML
    with open(fn_project_dirs, 'r') as file:
        project_dirs = yaml.safe_load(file)
        
    return project_dirs    
 
# Function to match parameters of files against list of predefined parameters
def match_parameters(parameters, header_row):

    # Convert all parameter keys to lowercase
    parameters_lower = {key.lower(): value for key, value in parameters.items()}
    
    # Find matching parameters and their formats
    matching_parameters = []
    descriptions = []
    units = []
    formats = []
    
    for header in header_row:
        lower_header = header.lower()
        if lower_header in parameters_lower:
            matching_parameters.append(header)
            descriptions.append(parameters_lower[lower_header]['description'])
            units.append(parameters_lower[lower_header]['unit'])
            formats.append(parameters_lower[lower_header]['format'])
        else:
            raise ValueError(f'parameter "{lower_header}" not found in file with tso_parameters')
            
    return matching_parameters, descriptions, units, formats

#%%

def update_format_missing_locs(format_list):
    
    """
    Update format strings in the provided list to handle missing location values appropriately.

    Args:
    - format_list (list of str): A list of format strings.

    Returns:
    - list of str: Updated list of format strings where format strings starting from the 5th 
                   element (index 4) are modified to handle missing values.
                   Specifically, if a format string contains '%d', it is updated to '%<n>d'
                   where <n> is the original number in the format string.

    The first four elements in the list are returned unchanged.
    """
    
    updated_formats = []
    
    for i, format_str in enumerate(format_list):
        if i < 4:
            # Keep the first four format strings unchanged
            updated_formats.append(format_str)
        else:
            # For the remaining format strings, look for patterns like '%d' and modify them
            match = re.search(r'%(\d+)', format_str)
            if match:
                n = match.group(1)
                format_str = f'%{n}d'
            updated_formats.append(format_str)
    
    return updated_formats

def update_format_missing_parameters(format_list, columns_with_only_na):
    
    """
    Update format strings in the provided list to handle missing parameter values appropriately,
    considering specific columns that only contain missing values.

    Args:
    - format_list (list of str): A list of format strings.
    - columns_with_only_na (list of int): A list of indices representing columns that only contain
                                          missing values.

    Returns:
    - list of str: Updated list of format strings where format strings starting from the 5th
                   element (index 4) are modified to handle missing values for specific columns.
                   If the column index is in `columns_with_only_na` and the format string contains 
                   '%d', it is updated to '%<n>d' where <n> is the original number in the format string.

    The first four elements in the list are returned unchanged.
    """
    
    updated_formats = []
    
    for i, format_str in enumerate(format_list):
        if i < 4:
            # Keep the first four format strings unchanged
            updated_formats.append(format_str)
        elif i not in columns_with_only_na:
            # If the column is not in the list of columns with only missing values, keep it unchanged
            updated_formats.append(format_str)
        else:
            # For columns with only missing values, modify the format strings to handle them
            match = re.search(r'%(\d+)', format_str)
            if match:
                n = match.group(1)
                format_str = f'%{n}d'
            updated_formats.append(format_str)
    
    return updated_formats

#%%

# Function to print the measurement info of the output file
def print_measurement_info(measurement_info):
    
    measurement_info_header = ''
    for label, item in measurement_info.items():
        if label == 'meetdatum':
            measurement_date = item[0].strftime('%Y%m%d')
            item = item[0].strftime('%d-%m-%Y %H:%M:%S')
        elif label == 'loc_names':
            continue
        elif len(item) == 1:
            item = item[0]
            if label != 'waarnemer(s)':
                item = item.upper()
        
        # # If item not empty
        # if item:
        #     measurement_info_header += f'# {label.upper():20s} : {item}\n'
    
        measurement_info_header += f'# {label.upper():20s} : {item}\n'    

    
    measurement_info_header += f'#\n* {measurement_date}\n'
    
    # The number of zeros (now set at 20) represents the number of columns to read with Map2009
    # actual number of columns can be smaller, but not larger
    measurement_info_header += '$ 9999 [00000000000000000000]\n'
    
    return measurement_info_header

# Function to print the header of the output file
def print_description_header(parameters, descriptions, units):

    description_header = ''
    for parameter, description, unit in zip(parameters, descriptions, units):
        description_header += f'>{parameter:10s} {description:20s} {unit:8s}\n'
    return description_header
    
#%%

def update_filelist_with_missing_locs(location, filelist, measurement_info):
    
    mpnaam = MPNaam(location)
    
    # Expected locations
    expected_locs = set(mpnaam.df['loc_num'].astype(int))
    
    # Found locations
    found_locs = set(filelist['loc_num'].astype(int))
    
    # Missing locations
    missing_locs = sorted(expected_locs - found_locs)   
                 
    if location.lower() == 'veer':
        for loc in [18, 19, 20]:
            if loc in missing_locs:
                missing_locs.remove(loc)
   
    # Print filenames for missing individual files
    missing_fn_short = [f'F{x:04d}' for x in missing_locs]
    
    # Make dataframe for missing locations dataframe
    missing_locs_df = pd.DataFrame({
        'filename': [np.nan] * len(missing_locs),
        'loc_num': missing_locs,
        'fn_short': missing_fn_short,
    })
    
    # Set loc_num to string for all locations
    missing_locs_df['loc_num'] = missing_locs_df['loc_num'].astype(str)
    
    # missing_locs_df.set_index('loc_num', inplace=True)
    
    # Add missing locations to filelist if there are any
    if missing_locs:
        filelist = pd.concat([filelist, missing_locs_df], ignore_index=True)
        filelist.sort_values(by='fn_short', inplace=True)
        filelist.reset_index(drop=True, inplace=True)
        
    # Filter filelist on locations in mpnaam
    filelist['loc_num'] = filelist['loc_num'].astype(int)
    locs_in_filelist = filelist['loc_num']   
    locs_in_mpnaam = mpnaam.df['loc_num']
        
    # Filter and sort the filelist
    filtered_filelist = filelist[locs_in_filelist.isin(locs_in_mpnaam)].sort_values(by='loc_num').reset_index(drop=True)
    filelist['loc_num'] = filelist['loc_num'].astype(str)
            
    return filtered_filelist, missing_locs_df['loc_num'].values.tolist()

#%%

def create_individual_dataframes(location, filelist, missing_locs, header_rows, loc_empty_line):

    df_dict = {}
    df_dict_missing = {}
    
    mpnaam = MPNaam(location)
    max_depth = pd.concat([mpnaam.df['loc_name'], mpnaam.df['max_depth']], axis=1)
    
    # Get the length of all headers per key
    length_headers = pd.DataFrame([[key, len(value)] for key, value in header_rows.items()])
    key_longest_headers = length_headers[0][length_headers[1].idxmax()]
    
    # Get list of unique columns from the longest header_row (make copy because of backpropagation of changes)
    unique_columns = header_rows[key_longest_headers].copy()

    # Iterate through the dictionary values and update the set with unique columns
    set_unique_columns = set(unique_columns)
    for columns in header_rows.values():        
        set_unique_columns.update(columns)
        
    # Append the items that are in unique_columns_set but not in header_row_output
    remaining_columns = [col for col in set_unique_columns if col not in unique_columns]
    unique_columns.extend(remaining_columns)
        
    header_row_output = unique_columns
    header_row_output.remove('DATUM')
    
    # Filter filelist on locations in mpnaam
    locs_in_mpnaam = mpnaam.df['loc_num']
    locs_in_filelist = filelist['loc_num']
    filtered_filelist = filelist[locs_in_filelist.isin(locs_in_mpnaam)]
        
    for i, row in filtered_filelist.iterrows():                         
    
        # Get filename without dirs and extension
        loc_num = str(row['loc_num'])
        filename = row['filename']
        fn_short = row['fn_short']   
        
        max_depth_loc = max_depth['max_depth'].loc[int(loc_num)]
    
        # Make dataframe if file from individual location is missing
        if loc_num in missing_locs:            
    
            depth_range = np.array([0, max_depth_loc])
            # nan_vector = np.full(len(depth_range), np.nan)
            
            time_missing = np.repeat(2359, len(depth_range))
            x_missing = np.repeat([mpnaam.df.loc[int(loc_num)]['x']], len(depth_range))
            y_missing = np.repeat([mpnaam.df.loc[int(loc_num)]['y']], len(depth_range))
    
            # df_missing dataframe for missing locations dataframe
            df_missing = pd.DataFrame(columns=header_row_output)
            df_missing[df_missing.columns[0:4]] = np.array([x_missing, y_missing, time_missing, depth_range]).T  
            df_missing[df_missing.columns[4:]] = 9999     
            df_missing['TIJD'] = df_missing['TIJD'].astype(int)
            df_missing['TIJD'] = df_missing['TIJD'].apply(lambda x: f'{int(x):04d}')
            df_dict_missing[fn_short] = df_missing

        else:
    
            # Read individual datafile into dictionary
            df = pd.read_csv(filename, sep='\\s+', skiprows=loc_empty_line[fn_short]+2, header=None, na_values='--')
            
            # Set NaN to 9999
            df = df.fillna(9999)
            
            # print(f'Aantal kolommen in dataframe = {len(df.columns)}, aantal headerkolommen = {len(header_rows[i])}')
            df.columns = header_rows[fn_short]
            
            # Identify missing columns in df using sets
            missing_columns = set(header_row_output) - set(df.columns)
            
            # Add missing columns with 9999 values
            for col in missing_columns:
                df[col] = 9999
             
            # Drop column 'DATUM'
            df = df.drop(columns = 'DATUM')
            
            # Reorder the columns according to header_row_output
            df = df.reindex(columns=header_row_output)
            
            df['TIJD'] = df['TIJD'].round(-2) // 100
            df['TIJD'] = df['TIJD'].apply(lambda x: f'{int(x):04d}')
            
            df['x_mp'] = np.repeat(mpnaam.df['x'][mpnaam.df['loc_num'] == int(loc_num)].values, len(df))
            df['y_mp'] = np.repeat(mpnaam.df['y'][mpnaam.df['loc_num'] == int(loc_num)].values, len(df))
            df['diff_x'] = df['X'] - df['x_mp']
            df['diff_y'] = df['Y'] - df['y_mp']
            df_dict[fn_short] = df

    # Raise ValueError if no files are found
    if not df_dict:
        raise ValueError(f'No individual files found for "{location.upper()}", no file has been generated')
    
    return df_dict, df_dict_missing

#%% Write to file

def write_datafile(location, filelist, measurement_date='latest'):
    
    # Get MPNaam for order of output files
    mpnaam = MPNaam(location)
    
    # Read individual datafiles
    measurement_info, header_rows, loc_empty_line = read_individual_files(location, filelist)
    
    # Update filelist with missing locations
    filelist_with_missing_locs, missing_locs = update_filelist_with_missing_locs(location, filelist, measurement_info) 

    # Create dataframes for all individual locations
    df_dict, df_dict_missing = create_individual_dataframes(location, filelist_with_missing_locs, missing_locs, header_rows, loc_empty_line)
    
    # Load parameters from YAML-file
    parameters = load_tso_parameters()
    
    # Load output directory from YAML-file
    project_dirs = load_project_dirs()
    output_dir = pathlib.Path(project_dirs.get('output_dir'))
    data_dir = pathlib.Path(project_dirs.get('data_dir'))
    
    # Filename of datafile    
    filename = find_dirname_measurement(location, measurement_date)
    location_code, _, location_data = get_location_info(location)
    measurement_date_out = measurement_info['meetdatum'][0].strftime('%Y%m%d')
    filename = output_dir / f'{location_code}{measurement_date_out}'
    
    # Ensure the output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Sort list with files based on location_order
    # Create a DataFrame from the location_order
    location_order_df = pd.DataFrame({'loc_num': mpnaam.location_order[location].astype(int), 'order': range(len(mpnaam.location_order[location]))})
    location_order_df = location_order_df.set_index('loc_num')
    
    # Merge the filelist with the location order DataFrame
    filelist_with_missing_locs = filelist_with_missing_locs.merge(location_order_df, on='loc_num', how='left')
    filelist_with_missing_locs = filelist_with_missing_locs.sort_values('order')       

    with open(filename, 'w') as f:
    
        for i, row in filelist_with_missing_locs.iterrows():
    
            loc_num = str(row['loc_num'])
            key = row['fn_short']
    
            if loc_num in missing_locs:  
                output_data = df_dict_missing[key]     
            else:                 
                output_df = df_dict[key]
                output_data = output_df[output_df.columns[:-4]]               
                              
            # Move column 'TIJD' to first position
            columns = output_data.columns.tolist()
            columns.insert(0, columns.pop(columns.index('TIJD')))
            output_data = output_data[columns]
            output_data['TIJD'] = output_data['TIJD'].astype(str)        
            
            # Find matching parameters and output formats
            matching_parameters, descriptions, units, formats = match_parameters(parameters, output_data.columns.to_list())
            
            # Update formats for missing locations because of NaN/9999 output
            if loc_num in missing_locs:
                formats = update_format_missing_locs(formats)
                
            # Find locations with missing parameters and update formatting because of NaN/9999 output    
            columns_with_only_na = [index for index, col in enumerate(output_data.columns) if (output_data[col] == 9999).all()]
            
            if columns_with_only_na:
                formats = update_format_missing_parameters(formats, columns_with_only_na)                   
                
            output_data = output_data.to_numpy()
            
            # Execute functions to gather measurement_info_header and description_header
            measurement_info_header = print_measurement_info(measurement_info)
            description_header = print_description_header(matching_parameters, descriptions, units)
            
            if i == filelist_with_missing_locs.index[0]:
                f.write(measurement_info_header)
                f.write(description_header)
            
            output_data[0,0] = f'-{output_data[0,0]}'
            np.savetxt(f, output_data, fmt=formats)
            data_dir_location = data_dir / f'{location}'
            
    print(f'File "{filename}" created, move file to directory "{data_dir_location}" for creating plots')