import glob
import os
import re
import datetime
import numpy as np
import pandas as pd
import yaml
import pyproj

from collections import Counter
from pathlib import Path
from pyproj import Transformer

#%% Load all files

def generate_filelist(location, measurement_date='latest'):
    
    _, _, location_data, _ = get_location_info(location)
    
    project_dirs = load_project_dirs()
    indiv_files_dir = Path(project_dirs.get('individual_files_dir'))
    
    workdir = indiv_files_dir / f'{location_data}'
    all_dirs = sorted(glob.glob(str(workdir)+'/*/', recursive = True))
    
    # Filter all map names where the last part has 8 digits, e.g. 20240401
    filtered_dirs = [d for d in all_dirs if re.match(r'^.*[\\/]\d{8}', d)]
    filtered_dirs = pd.DataFrame(filtered_dirs, columns=['Meting_id'])
    
    # Find measurement date in list of directories, e.g. '20240601'
    dirname = find_dirname_measurement(location, measurement_date)
        
    # List of all files from most recent directory    
    filetype = determine_filetype(dirname)
    
    if filetype == 'dat':    
        filelist = sorted(glob.glob(os.path.join(dirname, 'F*.dat')))
    elif filetype == 'csv':
        filelist = sorted(glob.glob(os.path.join(dirname, '*.csv')))
        
    filelist = pd.DataFrame(filelist, columns=['filename'])
    
    if filetype == 'dat':
        filelist['fn_short'] = filelist['filename'].apply(lambda x: Path(x).stem)
    
        # Assumption is that filenames F**** have been generated by rename_raw_tso_files
        # First convert number to integer without leading zeros, then to string
        filelist['loc_num'] = filelist['fn_short'].str.strip('F').values.astype(int).astype(str)
        
    elif filetype == 'csv':
        
        # Assumption is that filenames <donar_code-nn_date> have been generated by rename_raw_tso_files
        filelist['fn_short'] = filelist['filename'].apply(lambda x: Path(x).stem.split('_', 1)[0])
        filelist['loc_num'] = filelist['fn_short'].apply(lambda x: Path(x).stem.split('-', 1)[1])     
 
    return filelist

#%% Function to get all directories with indiviudal measurement files

def get_all_datafiles(location, plot_mode='single'):
    
    # Get location code
    location_code, _ , location_data, _ = get_location_info(location)    
   
    project_dirs = load_project_dirs()
    data_dir = Path(project_dirs.get('data_dir'))
    
    # Grevelingen Noord is in same directory as Grevelingen
    if location == 'grno':
        workdir_vert = data_dir / f'{location_data}/'
    else:
        workdir_vert = data_dir / f'{location}/'
    
    # Find all datafiles; for plot_mode 'single', include files that might have extra characters after the date, 
    # like '_03' when only three measurements are available
    if plot_mode == 'single':
        all_datafiles = glob.glob(str(workdir_vert) + f'/{location_code}????????*')
    else:
        all_datafiles = glob.glob(str(workdir_vert) + f'/{location_code}????????')
    
    filelist_datafiles = pd.DataFrame(data=all_datafiles, columns=['Meting']).sort_values(by='Meting').reset_index(drop=True)
    filelist_datafiles['Meting_id'] = filelist_datafiles['Meting'].apply(lambda x: Path(x).stem)
    
    # Define the valid pattern: location_code followed by exactly 8 digits, optionally followed by additional characters.
    valid_pattern = re.compile(rf"^{location_code}\d{{8}}.*$")
    
    # Filter the dataframe to keep only files that match the expected pattern.
    filelist_datafiles = filelist_datafiles[filelist_datafiles['Meting_id'].str.match(valid_pattern)]
    
    if filelist_datafiles.empty:
        raise ValueError(f'Geen datafiles gevonden voor locatie "{location_code}" in map "{workdir_vert}"')
    
    return filelist_datafiles

#%% Function to get all directories with indiviudal measurement files

def get_measurement_dirs(location):
        
    # Get location code
    _, _, location_data, _ = get_location_info(location)
    
    # Load location of individual files
    project_dirs = load_project_dirs()
    individual_files_dir = Path(project_dirs.get('individual_files_dir'))

    workdir_individual_files = individual_files_dir / f'{location_data}/'
    
    # Find all datafiles
    all_dirs = glob.glob(str(workdir_individual_files) + '/????????*/')
    list_measurement_dirs = pd.DataFrame(data=all_dirs, columns=['Meting']).sort_values(by='Meting').reset_index(drop=True)
    list_measurement_dirs['Meting_id'] = list_measurement_dirs['Meting'].apply(lambda x: Path(x).stem)
    
    return list_measurement_dirs

#%% Function to find the name of a directory of a certain measurement

def find_dirname_measurement(location, measurement_date='latest'):
    list_measurement_dirs = get_measurement_dirs(location)
    
    if measurement_date == 'latest':
        
        # Use the last entry
        dirname = list_measurement_dirs['Meting'].iloc[-1]
    
    elif isinstance(measurement_date, int):
        
        # Use the integer as an index (this supports negative indices as well)
        try:
            dirname = list_measurement_dirs['Meting'].iloc[measurement_date]
        except IndexError as e:
            raise ValueError(f"Index {measurement_date} is out of range for the measurement directories.") from e
    
    else:
        try:
            # Assume measurement_date is a string to be matched in the 'Meting_id' column
            matched_dirs = list_measurement_dirs['Meting'][list_measurement_dirs['Meting_id'] == f'{measurement_date}']
            num_matches = len(matched_dirs)
            
            if num_matches == 0:
                raise ValueError(f'No matches found for "{measurement_date}"')
            elif num_matches > 1:
                raise ValueError(f'Multiple matches found for "{measurement_date}"')
        except ValueError as e:
            print(e)            
            raise
        
        # If a match was found, extract the directory name
        dirname = matched_dirs.iloc[0]
    
    return dirname

#%% Function to find the filename of a datafile

def find_filename_datafile(location, measurement_date='latest', plot_mode='single'):
    
    filelist_datafiles = get_all_datafiles(location, plot_mode)
    
    if measurement_date == 'latest':
        filename_datafile = filelist_datafiles['Meting'].iloc[-1]
    
    # Check if the measurement_date is an integer (e.g. -2 or -3)
    elif isinstance(measurement_date, (int, np.integer)):
        try:
            # Handle negative or positive indices
            if len(filelist_datafiles) >= np.abs(measurement_date):
                filename_datafile = filelist_datafiles['Meting'].iloc[measurement_date]
            else:
                raise IndexError(f'Index {measurement_date} is out of bounds for available files.')
        except IndexError as e:
            print(e)
            return None   
    
    # Match the string against the meting_id column 
    else:      
        try:                       
            location_code, _ , _ = get_location_info(location)
            filename_datafile = filelist_datafiles['Meting'][filelist_datafiles['Meting_id'] == f'{location_code}{measurement_date}']
            
            # Check the number of matches
            num_matches = len(filename_datafile)
            
            if num_matches == 0:
                raise ValueError(f'No matches found for "{measurement_date}"')
            elif num_matches > 1:
                raise ValueError(f'Multiple matches found for "{measurement_date}"')
    
        except ValueError as e:
            print(e)
            return None
        
        # Get string from series
        filename_datafile = filename_datafile.iloc[0]
            
    return filename_datafile

#%% Function to get location code or location name from location

def get_location_info(location):
    
    # Load data from YAML file
    with open('locaties.yaml', 'r') as file:
        df = pd.DataFrame(yaml.safe_load(file)['locations'])
    
    # Filter the DataFrame for the given location name
    result = df[df['location'] == location]
    location_code = result['location_code'].values[0]
    location_name = result['location_name'].values[0]
    location_data = result['location_data'].values[0]
    donar_code = result['donar_code'].values[0]
    
    if not result.empty:
        return location_code, location_name, location_data, donar_code
    else:
        raise ValueError(f'File mpnaam of {location} not found')        

# %% Function to determine the filetype of the source files (dat or csv)
           
def determine_filetype(directory):
    
    """Return 'dat' or 'csv' depending on which type is present

    Raise ValueError only when both .dat and .csv files are found
    Ignore any other extensions
    """
      
    files = [p for p in Path(directory).iterdir() if p.is_file()]
    ext_counts = Counter(p.suffix.lower() for p in files)

    # Bools to determine presence of both filetypes
    has_dat = '.dat' in ext_counts
    has_csv = '.csv' in ext_counts

    if has_dat and has_csv:
        raise ValueError(f'Both .dat and .csv files found in {directory}; ext_counts={dict(ext_counts)}')
    elif has_dat:
        return 'dat'
    elif has_csv:
        return 'csv'
    else:
        # Neither file type present: decide whether to raise or return None
        raise ValueError(f'No .dat or .csv files found in {directory}; ext_counts={dict(ext_counts)}')

# %%

class MPNaam:
    def __init__(self, location):
        
        # Set location
        self.location = location
        
        # Retrieve location data
        self.location_code, self.location_name, self.location_data, self.donar_code = get_location_info(self.location)
        
        # Read data into dataframe
        self.df = self.read_mpnaam()

    def read_mpnaam(self):       
        
        # Load directory for individual files
        project_dirs = load_project_dirs()
        indiv_files_dir = Path(project_dirs.get('individual_files_dir'))
        
        # Load measurement points from CSV file
        df = pd.read_csv(indiv_files_dir / f'{self.location_data}/mpnaam', header=None, sep='\\s+', names=['x', 'y', 'max_depth', 'loc_name'])
        
        # Convert values to string (if not already string from reading csv)
        df['loc_name'] = df['loc_name'].astype(str)       
              
        if self.location in ('grev', 'grno'):
            
            # Drop 'DREI', will be corrected later in read_datafile
            # if 'DREI' is found instead of 'DREIS'            
            df = df[df['loc_name'] != 'DREI'].reset_index(drop=True)
            
            if self.location == 'grev':
                df = df.iloc[:20]
            else:
                df = df.iloc[20:]                
            
        # Set distances for VZM locations
        if self.location in ('anka', 'volk', 'zoom'):
            
            # Create dictionary with location key and location order
            location_order = {
                'anka': np.array([37, 39, 40, 41, 42, 43, 44, 45, 46]).astype(str),
                'volk': np.array([15, 16, 13, 12, 11, 10, 9, 7, 4, 1]).astype(str),
                'zoom': np.array([27, 29, 30, 34, 33, 32, 31, 36, 37, 38]).astype(str)
            }            
            
            # Filter for locations that are in use
            df = df[df['loc_name'].isin(location_order[self.location])]
            
            # Create a sorting DataFrame
            sort_df = pd.DataFrame({'loc_name': location_order[self.location], 'order': range(len(location_order[self.location]))})
            sort_df = sort_df.set_index('loc_name')
            
            # Merge with the sorting DataFrame
            df = df.merge(sort_df, on='loc_name', how='left')
            
            # Sort based on the 'order' column
            df = df.sort_values('order')  
            
            # Insert location numbers
            df.insert(3, 'loc_num', np.array(location_order[self.location]).astype(int))
            
            self.location_order = location_order
            
        # Insert location numbers for all other locations    
        if not self.location in ('anka', 'volk', 'zoom'):
        
            start_mp_num = int(df['loc_name'].iloc[0])
            end_mp_num = start_mp_num + df.shape[0]
            
            self.location_order = {self.location: np.arange(start_mp_num, end_mp_num)}
            df.insert(3, 'loc_num', np.arange(start_mp_num, end_mp_num))
                    
        # Calculate distances between measurement points
        df['delta'] = np.sqrt(df['x'].diff().abs()**2 + df['y'].diff().abs()**2)
        df['dist'] = df['delta'].cumsum().round()
        
        # Replace NaNs by zeros for interpolation to work
        df[['delta', 'dist']] = df[['delta', 'dist']].fillna(0)
        df.index = df['loc_num']

        # Convert RD New coordinates to ETRS89 / UTM zone 31N
        transformer = pyproj.Transformer.from_crs('EPSG:28992', 'EPSG:25831', always_xy=True)
        df[['x_etrs', 'y_etrs']] = df.apply(
            lambda row: pd.Series(transformer.transform(row['x'], row['y'])), axis=1)
        
        # Convert RD New coordinates to WGS84
        transformer = pyproj.Transformer.from_crs('EPSG:28992', 'EPSG:4326', always_xy=True)
        df[['x_wgs84', 'y_wgs84']] = df.apply(
            lambda row: pd.Series(transformer.transform(row['x'], row['y'])), axis=1)

        return df

# Example usage:
# mpnaam = MPNaam('grev')
# print(mpnaam.df)

#%%

# Function that loads all predefined parameters
def load_tso_parameters():

    fn_tso_parameters = 'tso_parameters.yaml' 

    # Load the TSO parameters from YAML file
    with open(fn_tso_parameters, 'r') as file:
        parameters = yaml.safe_load(file)
        
    return parameters

# Function that loads the project directories
def load_project_dirs():
    
    fn_project_dirs = 'project_dirs.yaml'
    
    # Load directory paths from YAML
    with open(fn_project_dirs, 'r') as file:
        project_dirs = yaml.safe_load(file)
        
    return project_dirs    
 
# Function to match parameters of files against list of predefined parameters
def match_parameters(parameters, header_row):

    # Convert all parameter keys to lowercase
    parameters_lower = {key.lower(): value for key, value in parameters.items()}
    
    # Find matching parameters and their formats
    matching_parameters = []
    descriptions = []
    units = []
    formats = []
    
    for header in header_row:
        lower_header = header.lower()
        if lower_header in parameters_lower:
            matching_parameters.append(header)
            descriptions.append(parameters_lower[lower_header]['description'])
            units.append(parameters_lower[lower_header]['unit'])
            formats.append(parameters_lower[lower_header]['format'])
        else:
            raise ValueError(f'parameter "{lower_header}" not found in file with tso_parameters')
            
    return matching_parameters, descriptions, units, formats

#%%

def update_format_missing_locs(format_list):
    
    """
    Update format strings in the provided list to handle missing location values appropriately.

    Args:
    - format_list (list of str): A list of format strings.

    Returns:
    - list of str: Updated list of format strings where format strings starting from the 5th 
                   element (index 4) are modified to handle missing values.
                   Specifically, if a format string contains '%d', it is updated to '%<n>d'
                   where <n> is the original number in the format string.

    The first four elements in the list are returned unchanged.
    """
    
    updated_formats = []
    
    for i, format_str in enumerate(format_list):
        if i < 4:
            # Keep the first four format strings unchanged
            updated_formats.append(format_str)
        else:
            # For the remaining format strings, look for patterns like '%d' and modify them
            match = re.search(r'%(\d+)', format_str)
            if match:
                n = match.group(1)
                format_str = f'%{n}d'
            updated_formats.append(format_str)
    
    return updated_formats

def update_format_missing_parameters(format_list, columns_with_only_na):
    
    """
    Update format strings in the provided list to handle missing parameter values appropriately,
    considering specific columns that only contain missing values.

    Args:
    - format_list (list of str): A list of format strings.
    - columns_with_only_na (list of int): A list of indices representing columns that only contain
                                          missing values.

    Returns:
    - list of str: Updated list of format strings where format strings starting from the 5th
                   element (index 4) are modified to handle missing values for specific columns.
                   If the column index is in `columns_with_only_na` and the format string contains 
                   '%d', it is updated to '%<n>d' where <n> is the original number in the format string.

    The first four elements in the list are returned unchanged.
    """
    
    updated_formats = []
    
    for i, format_str in enumerate(format_list):
        if i < 4:
            # Keep the first four format strings unchanged
            updated_formats.append(format_str)
        elif i not in columns_with_only_na:
            # If the column is not in the list of columns with only missing values, keep it unchanged
            updated_formats.append(format_str)
        else:
            # For columns with only missing values, modify the format strings to handle them
            match = re.search(r'%(\d+)', format_str)
            if match:
                n = match.group(1)
                format_str = f'%{n}d'
            updated_formats.append(format_str)
    
    return updated_formats

#%% Function to print the measurement info of the output file

def print_measurement_info(measurement_info):
    
    measurement_info_header = ''
    for label, item in measurement_info.items():
        if label == 'meetdatum':          
            measurement_date = datetime.datetime.strptime(item[0], '%Y-%m-%d %H:%M:%S').strftime('%Y%m%d')
            item = datetime.datetime.strptime(item[0], '%Y-%m-%d %H:%M:%S').strftime('%d-%m-%Y %H:%M:%S')
        elif label == 'loc_names':
            continue
        elif len(item) == 1:
            item = item[0]
            if label != 'waarnemer(s)':
                item = item.upper()
        
        # # If item not empty
        # if item:
        #     measurement_info_header += f'# {label.upper():20s} : {item}\n'
    
        measurement_info_header += f'# {label.upper():20s} : {item}\n'    
    
    measurement_info_header += f'#\n* {measurement_date}\n'
    
    # The number of zeros (now set at 20) represents the number of columns to read with Map2009
    # actual number of columns can be smaller, but not larger
    measurement_info_header += '$ 9999 [00000000000000000000]\n'
    
    return measurement_info_header

# Function to print the header of the output file
def print_description_header(parameters, descriptions, units, add_chloride=False):

    description_header = ''
    for parameter, description, unit in zip(parameters, descriptions, units):
        description_header += f'>{parameter:10s} {description:20s} {unit:8s}\n'
    
    # Add chloride to header if chloride is calculted based on salinity
    if add_chloride == True:
        description_header += '>CL-        Chloride             mg/l'
        
    return description_header
    
#%%

def update_filelist_with_missing_locs(location, filelist, mpnaam, measurement_info):
   
    # Expected locations
    expected_locs = set(mpnaam.df['loc_num'].astype(int))
    
    # Found locations
    found_locs = set(filelist['loc_num'].astype(int))
    
    # Missing locations
    missing_locs = sorted(expected_locs - found_locs)   
                 
    if location.lower() == 'veer':
        for loc in [18, 19, 20]:
            if loc in missing_locs:
                missing_locs.remove(loc)
   
    # Get file extension of first file in filelist
    filetype = Path(filelist.iloc[0]['filename']).suffix
    
    # Print filenames for missing individual files
    if filetype == '.dat':    
        missing_fn_short = [f'F{x:04d}' for x in missing_locs]
    elif filetype == '.csv':
        donar_code = mpnaam.donar_code.upper()
        missing_fn_short = [f'{donar_code}-{x:02d}' for x in missing_locs]
    
    # Make dataframe for missing locations dataframe
    missing_locs_df = pd.DataFrame({
        'filename': [np.nan] * len(missing_locs),
        'loc_num': missing_locs,
        'fn_short': missing_fn_short
        })
    
    # Set loc_num to string for all locations
    missing_locs_df['loc_num'] = missing_locs_df['loc_num'].astype(str)
    
    # missing_locs_df.set_index('loc_num', inplace=True)
    
    # Add missing locations to filelist if there are any
    if missing_locs:
        filelist = pd.concat([filelist, missing_locs_df], ignore_index=True)
        filelist.sort_values(by='fn_short', inplace=True)
        filelist.reset_index(drop=True, inplace=True)
        
    # Filter filelist on locations in mpnaam
    filelist['loc_num'] = filelist['loc_num'].astype(int)
    locs_in_filelist = filelist['loc_num']   
    locs_in_mpnaam = mpnaam.df['loc_num']
        
    # Filter and sort the filelist
    filtered_filelist = filelist[locs_in_filelist.isin(locs_in_mpnaam)].sort_values(by='loc_num').reset_index(drop=True)
    filelist['loc_num'] = filelist['loc_num'].astype(str)
            
    return filtered_filelist, missing_locs_df['loc_num'].values.tolist()

# %%

def read_individual_files(location, filelist, filetype):

    # Create empty variables
    fn_short = []    
    raw_text_files = []
    width_df = []
    header_rows = {}
    dt_measurements = {}
    loc_empty_line = {}
    measurement_date = 0

    # Define measurment info
    measurement_info = {
        'loc_names' : [],
        'meetvaartuig' : [],
        'meetdatum': [],
        'meetgebied' : [],
        'startpunt' : [],
        'eindpunt' : [],
        'projectcode' : [],
        'opdrachtgever' : [],
        'beheerder' : [],
        'bemonst. inst': [],
        'waarnemer(s)': [],
        }
    
    # Read all individual csv-files
    if filetype == 'csv':
        
        # Fill measurement info
        _, location_name, _, donar_code = get_location_info(location)
        measurement_info['meetgebied'] = location_name
        measurement_info['projectcode'] = donar_code.upper()
        measurement_info['opdrachtgever'] = 'Rijkswaterstaat Zee en Delta'
        measurement_info['beheerder'] = 'District Zeeuwse Wateren'
        measurement_info['bemonst. inst'] = 'Rijkswaterstaat CIV'
        
        for i, filename in enumerate(filelist['filename']):
            
            # Read only header of file
            fn_short = filelist['fn_short'].iloc[i]
            header_rows[fn_short] = pd.read_csv(filename, nrows=0, sep=';').columns.str.strip().tolist()
            first_row = pd.read_csv(filename, nrows=1, sep=';')
            
            # Date and time of measurement
            mdate = pd.to_datetime(first_row.iloc[0]['DATUM'], format='%Y%m%d')
            mtime = pd.to_datetime(first_row['TIJD'].astype(str).str.zfill(6).iat[0], format='%H%M%S')   
            mdatetime = mdate + (mtime - mtime.normalize())
            dt_measurements[fn_short] = mdatetime
            
            measurement_info['meetdatum'].append(mdatetime.strftime('%Y-%m-%d %H:%M:%S'))   
            
        # Get first and last location
        dt_locs = pd.Series(dt_measurements) 
        measurement_info['startpunt'] = dt_locs.idxmin()
        measurement_info['eindpunt'] = dt_locs.idxmax()
            
        return measurement_info, header_rows, loc_empty_line
        
    # Read all individual dat-files
    for i, filename in enumerate(filelist['filename']):
        
        fn_short = filelist['fn_short'].iloc[i]
    
        with open(filename, 'r') as f:
            raw_text = f.readlines()
            raw_text_files.append(raw_text)
    
            for num, line in enumerate(raw_text):
    
                # Set all strings to lower case for matching
                line = line.lower()
    
                # Number of measurement point
                if '#00 donarlocatie:' in line:                
                   
                    # Clean string to be able to append named locations
                    cleaned_line = line.replace('#00 donarlocatie:', '').strip().upper()
                    
                    # Exception for Grevelingen
                    if 'SCH' in cleaned_line:
                        cleaned_line = 'SCH'
                    
                    # Split number from string
                    measurement_info['loc_names'].append(cleaned_line.split('-')[-1])                   
                                        
                if '# vertikaal' in line:
                    # Split string, lose first 3 items, join last ones
                    measurement_info['meetvaartuig'].append(' '.join(line.split()[3:]).upper())
                    
                # Measurement date:
                if '*' in line:
                    datetime_string = line.split('*')[-1].strip()                     
                    try:
                        measurement_date = datetime.datetime.strptime(datetime_string, '%d-%m-%Y %H:%M:%S')
                        measurement_info['meetdatum'].append(measurement_date.strftime('%Y-%m-%d %H:%M:%S'))
                    except ValueError:
                        continue                                       
                 
                # Observer
                if '#08 waarnemer' in line:          
                    measurement_info['waarnemer(s)'].append((line.split(':')[-1]).strip().capitalize())
                    
                # Measurement location
                if '#21 meetgebied' in line:
                    measurement_info['meetgebied'].append((line.split(':')[-1]).strip().capitalize())
                    
                # Measurement location
                if '#22 startpunt' in line:
                    measurement_info['startpunt'].append((line.split(':')[-1]).strip())
                        
                # Measurement location
                if '#23 eindpunt' in line:
                    measurement_info['eindpunt'].append((line.split(':')[-1]).strip())
                    
                # Projectcode
                if '#06 projectcode' in line:
                    measurement_info['projectcode'].append((line.split(':')[-1]).strip())                        
                    
                # Measurement location
                if '#51 opdrachtgever' in line:
                    measurement_info['opdrachtgever'].append((line.split(':')[-1]).strip())                    

                # Measurement location
                if '#52 beheerder' in line:
                    measurement_info['beheerder'].append((line.split(':')[-1]).strip())          
    
                # Find location of first empy line
                if line in ['\n', '\r\n']:
                    loc_empty_line[fn_short] = num
    
                    # First character is a ~ which messes up column names
                    header_row = raw_text[loc_empty_line[fn_short]+1].split()[1:]
    
                    # Replace 'T' with 'Temp'
                    header_row = ['Temp' if item == 'T' else item for item in header_row]
                    width_df.append(len(header_row))
    
                    # Save header row of individual file
                    header_rows[fn_short] = header_row
    
                    # Break because empty line has been found and header rows extracted
                    break
    
            # # Check if headers are the same (now disabled because in VZM there can be a difference in number of columns)
            # if i > 0 and header_rows[i-1] != header_rows[i]:
            #     raise ValueError(f'Header row "{filelist[i]}" is not equal to header row "{filelist[i-1]}"')         

    # Count occurences and select most frequently mentioned one to display in combined datafile
    to_count = list(measurement_info.keys())
    
    for item in to_count:
        if item not in  ('loc_names', 'meetdatum') and measurement_info[item]:
            measurement_info[item] = pd.DataFrame(measurement_info[item]).value_counts().index[0]            
    
    return measurement_info, header_rows, loc_empty_line

# %%

def create_individual_dataframes(location, filelist, filetype, mpnaam, missing_locs, header_rows, loc_empty_line):

    # Allocate dictionaries
    df_dict = {}
    df_dict_missing = {}
    
    max_depth = pd.concat([mpnaam.df['loc_name'], mpnaam.df['max_depth']], axis=1)
    
    # Transformer for coördinates
    wgs84_to_rd = Transformer.from_crs(
        crs_from='EPSG:4326',     # WGS‑84 lat/lon
        crs_to='EPSG:28992',      # Amersfoort / RD New
        always_xy=True            # expect (lon, lat) order
        )
    
    # Get the length of all headers per key
    length_headers = pd.DataFrame([[key, len(value)] for key, value in header_rows.items()])
    key_longest_headers = length_headers[0][length_headers[1].idxmax()]
    
    # Get list of unique columns from the longest header_row (make copy because of backpropagation of changes)
    unique_columns = header_rows[key_longest_headers].copy()

    # Iterate through the dictionary values and update the set with unique columns
    set_unique_columns = set(unique_columns)
    
    for columns in header_rows.values():        
        set_unique_columns.update(columns)
        
    # Append the items that are in unique_columns_set but not in header_row_output
    remaining_columns = [col for col in set_unique_columns if col not in unique_columns]
    unique_columns.extend(remaining_columns)
        
    header_row_output = unique_columns
    header_row_output.remove('DATUM')
    
    if 'LUCHTDK' in header_row_output:
        header_row_output.remove('LUCHTDK')                          
    
    # Filter filelist on locations in mpnaam
    locs_in_mpnaam = mpnaam.df['loc_num']
    locs_in_filelist = filelist['loc_num']
    filtered_filelist = filelist[locs_in_filelist.isin(locs_in_mpnaam)]
        
    for i, row in filtered_filelist.iterrows():                         
    
        # Get filename without dirs and extension
        loc_num = str(row['loc_num'])
        filename = row['filename']
        fn_short = row['fn_short']   
        
        max_depth_loc = max_depth['max_depth'].loc[int(loc_num)]
    
        # Make dataframe if file from individual location is missing
        if loc_num in missing_locs:            
    
            depth_range = np.array([0, max_depth_loc])
            # nan_vector = np.full(len(depth_range), np.nan)
            
            time_missing = np.repeat(2359, len(depth_range))
            x_missing = np.repeat([mpnaam.df.loc[int(loc_num)]['x']], len(depth_range))
            y_missing = np.repeat([mpnaam.df.loc[int(loc_num)]['y']], len(depth_range))
    
            # df_missing dataframe for missing locations dataframe
            df_missing = pd.DataFrame(columns=header_row_output)
            df_missing[df_missing.columns[0:4]] = np.array([x_missing, y_missing, time_missing, depth_range]).T  
            df_missing[df_missing.columns[4:]] = 9999     
            df_missing['TIJD'] = df_missing['TIJD'].astype(int)
            df_missing['TIJD'] = df_missing['TIJD'].apply(lambda x: f'{int(x):04d}')
            df_dict_missing[fn_short] = df_missing

        else:
           
            # Read files into dataframe
            if filetype == 'dat':    
                df = pd.read_csv(filename, sep='\\s+', skiprows=loc_empty_line[fn_short]+2, header=None, na_values='--')
                df.columns = header_rows[fn_short]
                
                # Sensor height must be positive, but input files can vary
                # with all negative or all positive values, so set to absolute
                df['SENSHTE'] = df['SENSHTE'].abs()
                
                # Adjust units sensor height and (x,y)-coördinates
                if df['SENSHTE'].max() > 100:                    
                    df['SENSHTE'] = df['SENSHTE'] / 100    
                
            elif filetype == 'csv':
                
                # Get sample of file to determine decimal separator
                sample = Path(filename).read_text(errors='ignore')[:1024]
                
                # Count occurences
                comma = sample.count(',')
                dot   = sample.count('.')
                dec = ',' if comma > dot else '.'
                
                df = pd.read_csv(filename, decimal=dec, sep=';')
                df.columns = df.columns.str.strip()
                
                # Sensor height must be positive, but input files can vary
                # with all negative or all positive values, so set to absolute
                df['SENSHTE'] = df['SENSHTE'].abs()
                
                # Adjust units sensor height and (x,y)-coördinates
                if df['SENSHTE'].max() > 100:                    
                    df['SENSHTE'] = df['SENSHTE'] / 100                
                     
                df['X'], df['Y'] = wgs84_to_rd.transform(df['Y'], df['X'])
                
                # Filter all rows with only air pressure as valid measurment parameter
                if 'LUCHTDK' in df.columns:
                    position = df.columns.get_loc('LUCHTDK')
                    temp_df = df[df.columns[position+1:]]
                    rows_all_nan = temp_df.isna().all(axis=1)
                    df = df.loc[~rows_all_nan].drop(columns='LUCHTDK').reset_index(drop=True)
            
            # Add column with chloride data if salinity is present but chloride is not
            if 'SALNTT' in df.columns and not 'CL-' in df.columns:
                df = add_chloride_mg_per_l(df)
                print(f'{filename} - Added column with chloride data, based on salinity')
                
            # Set NaN to 9999
            df = df.fillna(9999)           
                        
            # Identify missing columns in df using sets
            missing_columns = set(header_row_output) - set(df.columns)           
            
            # Add missing columns with 9999 values
            for col in missing_columns:
                df[col] = 9999
                
            # Drop column 'DATUM'
            df = df.drop(columns = 'DATUM')
            
            # Reorder the columns according to header_row_output
            df = df.reindex(columns=header_row_output)
            
            df['TIJD'] = df['TIJD'].round(-2) // 100
            df['TIJD'] = df['TIJD'].apply(lambda x: f'{int(x):04d}')
            
            df['x_mp'] = np.repeat(mpnaam.df['x'][mpnaam.df['loc_num'] == int(loc_num)].values, len(df))
            df['y_mp'] = np.repeat(mpnaam.df['y'][mpnaam.df['loc_num'] == int(loc_num)].values, len(df))
            
            # Determine difference between actual coördinates and coördinates
            # of Donar location
            df['diff_x'] = df['X'] - df['x_mp']
            df['diff_y'] = df['Y'] - df['y_mp']
            
            # Place dataframe in dictionary
            df_dict[fn_short] = df                

    # Raise ValueError if no files are found
    if not df_dict:
        raise ValueError(f'No individual files found for "{location.upper()}", no file has been generated')
    
    return df_dict, df_dict_missing

# %% Functions to calculate chloride based on salinity if chloride not present

def seawater_density_kg_m3(S, T):
    
    """Seawater density at p≈0 dbar according to EOS-80 (kg/m^3).
    Inputs:
      S: Practical salinity in PSU (numerically ~ g/kg).
      T: Temperature in degrees Celsius.
    """
    
    # Density of pure water as a function of T (kg/m^3)
    rho_w = (999.842594
             + 6.793952e-2 * T
             - 9.095290e-3 * T**2
             + 1.001685e-4 * T**3
             - 1.120083e-6 * T**4
             + 6.536332e-9 * T**5)

    # Temperature-dependent coefficients for saline contribution
    # Linear in S.
    A = (0.824493
         - 4.0899e-3 * T
         + 7.6438e-5 * T**2
         - 8.2467e-7 * T**3
         + 5.3875e-9 * T**4)

    # S^(3/2) term (nonlinear mixing effect).
    B = (-5.72466e-3
         + 1.0227e-4 * T
         - 1.6546e-6 * T**2)

    # S^2 term (small correction, temperature-independent)
    C = 4.8314e-4

    # Total density
    out_col = rho_w + A * S + B * (S ** 1.5) + C * (S ** 2)    
    
    return out_col

def add_chloride_mg_per_l(df, temperature_col='Temp', sal_col='SALNTT', out_col='CL-', chloride_fraction=0.553):
    
    """
    Add chloride (mg/L) to the DataFrame based on temperature (T) and salinity (S).

    Parameters:
      df: Pandas DataFrame.
      temp_col: Column name with T in degrees Celsius.
      sal_col: Column name with S in PSU (~ g/kg).
      out_col: Output column name.
      chloride_fraction: Mass fraction of chloride in total dissolved salts.

    Method:
      rho = Seawater density (kg/m^3).
      Total salts (mg/L) = S (g/kg) * rho (kg/m^3) -> g/m^3 = mg/L.
      Chloride (mg/L) = chloride_fraction * S * rho.
    """
    
    S = df[sal_col]
    T = df[temperature_col]
    
    rho = seawater_density_kg_m3(S, T)  # kg/m^3

    cl_mg_l = chloride_fraction * S * rho  # mg/L
    df[out_col] = np.round(cl_mg_l)
    
    return df

#%% Write to file

def write_datafile(location, filelist, measurement_date='latest'):
    
    # Get MPNaam for order of output files
    mpnaam = MPNaam(location)
    location_code = mpnaam.location_code
    
    # Determine filetype
    dirname = find_dirname_measurement(location, measurement_date)
    filetype = determine_filetype(dirname)
    
    # Read individual datafiles
    measurement_info, header_rows, loc_empty_line = read_individual_files(location, filelist, filetype)   
    
    # Update filelist with missing locations
    filelist_with_missing_locs, missing_locs = update_filelist_with_missing_locs(location, filelist, mpnaam, measurement_info) 

    # Create dataframes for all individual locations
    df_dict, df_dict_missing = create_individual_dataframes(location, filelist_with_missing_locs, filetype, mpnaam, missing_locs, header_rows, loc_empty_line)
    
    # Load parameters from YAML-file
    parameters = load_tso_parameters()
    
    # Load output directory from YAML-file
    project_dirs = load_project_dirs()
    output_dir = Path(project_dirs.get('output_dir'))
    data_dir = Path(project_dirs.get('data_dir'))
    
    # Filename of datafile     
    measurement_date_str = min(measurement_info['meetdatum'])
    measurement_date_out = datetime.datetime.strptime(measurement_date_str, '%Y-%m-%d %H:%M:%S').strftime('%Y%m%d')
        
    filename = output_dir / f'{location_code}{measurement_date_out}'
    
    # Ensure the output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Sort list with files based on location_order
    # Create a DataFrame from the location_order
    location_order_df = pd.DataFrame({'loc_num': mpnaam.location_order[location].astype(int), 'order': range(len(mpnaam.location_order[location]))})
    location_order_df = location_order_df.set_index('loc_num')
    
    # Merge the filelist with the location order DataFrame
    filelist_with_missing_locs = filelist_with_missing_locs.merge(location_order_df, on='loc_num', how='left')
    filelist_with_missing_locs = filelist_with_missing_locs.sort_values('order')       

    with open(filename, 'w') as f:
    
        for i, row in filelist_with_missing_locs.iterrows():
    
            loc_num = str(row['loc_num'])
            key = row['fn_short']
    
            if loc_num in missing_locs:  
                output_data = df_dict_missing[key]     
            else:                 
                output_df = df_dict[key]
                output_data = output_df[output_df.columns[:-4]]               
                              
            # Move column 'TIJD' to first position
            columns = output_data.columns.tolist()
            columns.insert(0, columns.pop(columns.index('TIJD')))
            output_data = output_data[columns]
            output_data['TIJD'] = output_data['TIJD'].astype(str)        
            
            # Find matching parameters and output formats
            matching_parameters, descriptions, units, formats = match_parameters(parameters, output_data.columns.to_list())
            
            # Update formats for missing locations because of NaN/9999 output
            if loc_num in missing_locs:
                formats = update_format_missing_locs(formats)
                
            # Find locations with missing parameters and update formatting because of NaN/9999 output    
            columns_with_only_na = [index for index, col in enumerate(output_data.columns) if (output_data[col] == 9999).all()]
            
            if columns_with_only_na:
                formats = update_format_missing_parameters(formats, columns_with_only_na)     
                
            output_data = output_data.to_numpy()
            
            # Execute functions to gather measurement_info_header and description_header
            measurement_info_header = print_measurement_info(measurement_info)
            description_header = print_description_header(matching_parameters, descriptions, units)
            
            if i == filelist_with_missing_locs.index[0]:
                f.write(measurement_info_header)
                f.write(description_header)
            
            output_data[0,0] = f'-{output_data[0,0]}'
            np.savetxt(f, output_data, fmt=formats)
            data_dir_location = data_dir / f'{location}'
            
    print(f'File "{filename}" created, move file to directory "{data_dir_location}" for creating plots')